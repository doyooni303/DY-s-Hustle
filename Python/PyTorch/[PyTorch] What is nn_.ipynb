{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[PyTorch] What is nn?","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOJEXYrx9tvE2+oRIX8blZU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"bh-_UeZyFoD9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600581488426,"user_tz":-540,"elapsed":3786,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","%matplotlib inline\n","import random\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import scipy as sp\n","import sklearn as skl"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqzEnrsGFynK","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u-Ljx8xgFiQH","colab_type":"text"},"source":["We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file,\n","click the link at the top of the page.\n","\n","PyTorch provides the elegantly designed modules and classes `torch.nn <https://pytorch.org/docs/stable/nn.html>`_ ,\n","`torch.optim <https://pytorch.org/docs/stable/optim.html>`_ ,\n","`Dataset <https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset>`_ ,\n","and `DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`_\n","to help you create and train neural networks.\n","In order to fully utilize their power and customize\n","them for your problem, you need to really understand exactly what they're\n","doing. To develop this understanding, we will first train basic neural net\n","on the MNIST data set without using any features from these models; we will\n","initially only use the most basic PyTorch tensor functionality. Then, we will\n","incrementally add one feature from ``torch.nn``, ``torch.optim``, ``Dataset``, or\n","``DataLoader`` at a time, showing exactly what each piece does, and how it\n","works to make the code either more concise, or more flexible.\n","\n","**This tutorial assumes you already have PyTorch installed, and are familiar\n","with the basics of tensor operations.** (If you're familiar with Numpy array\n","operations, you'll find the PyTorch tensor operations used here nearly identical).\n","\n","MNIST data setup\n","----------------\n","\n","We will use the classic `MNIST <http://deeplearning.net/data/mnist/>`_ dataset,\n","which consists of black-and-white images of hand-drawn digits (between 0 and 9).\n","\n","We will use `pathlib <https://docs.python.org/3/library/pathlib.html>`_\n","for dealing with paths (part of the Python 3 standard library), and will\n","download the dataset using\n","`requests <http://docs.python-requests.org/en/master/>`_. We will only\n","import modules when we use them, so you can see exactly what's being\n","used at each point.\n","\n"]},{"cell_type":"code","metadata":{"id":"o4cimFymFiQI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600581583972,"user_tz":-540,"elapsed":2679,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["from pathlib import Path\n","import requests\n","\n","DATA_PATH = Path(\"data\")\n","PATH = DATA_PATH / \"mnist\"\n","\n","PATH.mkdir(parents=True, exist_ok=True)\n","\n","URL = \"http://deeplearning.net/data/mnist/\"\n","FILENAME = \"mnist.pkl.gz\"\n","\n","if not (PATH / FILENAME).exists():\n","        content = requests.get(URL + FILENAME).content\n","        (PATH / FILENAME).open(\"wb\").write(content)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RaIuX_r3FiQK","colab_type":"text"},"source":["This dataset is in numpy array format, and has been stored using pickle,\n","a python-specific format for serializing data.\n","\n"]},{"cell_type":"code","metadata":{"id":"JsIqZ7q5FiQK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600581617751,"user_tz":-540,"elapsed":1828,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["import pickle\n","import gzip\n","\n","with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n","        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SdjmrQ2yFiQN","colab_type":"text"},"source":["Each image is 28 x 28, and is being stored as a flattened row of length\n","784 (=28x28). Let's take a look at one; we need to reshape it to 2d\n","first.\n","\n"]},{"cell_type":"code","metadata":{"id":"n_d2Tzw8FiQN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1600581625592,"user_tz":-540,"elapsed":764,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"41f48144-a36e-4884-a873-54b0c24a83b7"},"source":["from matplotlib import pyplot\n","import numpy as np\n","\n","pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n","print(x_train.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(50000, 784)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"odMFv2Q-FiQQ","colab_type":"text"},"source":["PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to\n","convert our data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fF6JHFMtKTiR","colab_type":"text"},"source":["# DATA 분리"]},{"cell_type":"code","metadata":{"id":"qngQzLg1FiQR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"ok","timestamp":1600581690757,"user_tz":-540,"elapsed":738,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"5333cd2c-3634-4d07-988b-b26e0ade3aeb"},"source":["import torch\n","\n","x_train, y_train, x_valid, y_valid = map(\n","    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",")\n","n, c = x_train.shape\n","x_train, x_train.shape, y_train.min(), y_train.max()\n","print(x_train, y_train)\n","print(x_train.shape)\n","print(y_train.min(), y_train.max(),y_train.unique())"],"execution_count":6,"outputs":[{"output_type":"stream","text":["tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n","torch.Size([50000, 784])\n","tensor(0) tensor(9) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"gs6Xky0vKV_y","colab_type":"text"},"source":["# 하이퍼 파라미터 설정\n"]},{"cell_type":"code","metadata":{"id":"iQKElHlaGe28","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600582098661,"user_tz":-540,"elapsed":724,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["lr = 0.5  # learning rate\n","epochs = 10  # how many epochs to train for\n","bs=128\n","\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MXoLFlKAKapf","colab_type":"text"},"source":["# 아키텍쳐 설정 with lossfunction"]},{"cell_type":"code","metadata":{"id":"Zqcfxm8IH30f","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600582292759,"user_tz":-540,"elapsed":705,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["import math\n","class Mnist_Logistic(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n","        self.bias = nn.Parameter(torch.zeros(10))\n","\n","    def forward(self, xb):\n","        return xb @ self.weights + self.bias\n","\n","import torch.nn.functional as F\n","loss_func = F.cross_entropy"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XOAoJ6h_Kkwv","colab_type":"text"},"source":["# optimizer 설정"]},{"cell_type":"code","metadata":{"id":"UkpIXRJ7He-G","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600582294621,"user_tz":-540,"elapsed":734,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["def get_model():\n","    model = Mnist_Logistic()\n","    return model, optim.SGD(model.parameters(), lr=lr)\n","\n","model, opt = get_model()\n","\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TimNeSQ-KoBq","colab_type":"text"},"source":["# Data loader 이용"]},{"cell_type":"code","metadata":{"id":"9376LFwsFiRi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600582296234,"user_tz":-540,"elapsed":699,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QFmerPk9Ksk0","colab_type":"text"},"source":["# 학습실행"]},{"cell_type":"code","metadata":{"id":"_6ql_e7BH8y9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1600582359168,"user_tz":-540,"elapsed":6463,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"85ecd114-1f10-436a-d5d4-06927f9ef699"},"source":["for epoch in range(epochs):\n","    for xb, yb in train_dl:\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","    \n","    print(loss_func(model(xb), yb))\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["tensor(0.1431, grad_fn=<NllLossBackward>)\n","tensor(0.1412, grad_fn=<NllLossBackward>)\n","tensor(0.1396, grad_fn=<NllLossBackward>)\n","tensor(0.1382, grad_fn=<NllLossBackward>)\n","tensor(0.1370, grad_fn=<NllLossBackward>)\n","tensor(0.1359, grad_fn=<NllLossBackward>)\n","tensor(0.1350, grad_fn=<NllLossBackward>)\n","tensor(0.1341, grad_fn=<NllLossBackward>)\n","tensor(0.1333, grad_fn=<NllLossBackward>)\n","tensor(0.1326, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"koiMpiCUFiRo","colab_type":"text"},"source":["Thanks to Pytorch's ``nn.Module``, ``nn.Parameter``, ``Dataset``, and ``DataLoader``,\n","our training loop is now dramatically smaller and easier to understand. Let's\n","now try to add the basic features necessary to create effecive models in practice.\n","\n","# Add validation\n","-----------------------\n","\n","In section 1, we were just trying to get a reasonable training loop set up for\n","use on our training data.  In reality, you **always** should also have\n","a `validation set <https://www.fast.ai/2017/11/13/validation-sets/>`_, in order\n","to identify if you are overfitting.\n","\n","Shuffling the training data is\n","`important <https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks>`_\n","to prevent correlation between batches and overfitting. On the other hand, the\n","validation loss will be identical whether we shuffle the validation set or not.\n","Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n","\n","We'll use a batch size for the validation set that is twice as large as\n","that for the training set. This is because the validation set does not\n","need backpropagation and thus takes less memory (it doesn't need to\n","store the gradients). We take advantage of this to use a larger batch\n","size and compute the loss more quickly.\n","\n"]},{"cell_type":"code","metadata":{"id":"yH6yk-8LFiRo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600582869290,"user_tz":-540,"elapsed":722,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n","\n","valid_ds = TensorDataset(x_valid, y_valid)\n","valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZ3_gHa_FiRq","colab_type":"text"},"source":["We will calculate and print the validation loss at the end of each epoch.\n","\n","(Note that we always call ``model.train()`` before training, and ``model.eval()``\n","before inference, because these are used by layers such as ``nn.BatchNorm2d``\n","and ``nn.Dropout`` to ensure appropriate behaviour for these different phases.)\n","\n"]},{"cell_type":"code","metadata":{"id":"icRtquLZFiRq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1600583018828,"user_tz":-540,"elapsed":7081,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"2ba09a54-ee5b-4612-bc9f-46a06fd6d4cc"},"source":["model, opt = get_model()\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for xb, yb in train_dl:\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","    model.eval()\n","    with torch.no_grad():\n","        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n","\n","    print(epoch, valid_loss / len(valid_dl))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["0 tensor(0.3008)\n","1 tensor(0.2976)\n","2 tensor(0.2773)\n","3 tensor(0.2695)\n","4 tensor(0.2710)\n","5 tensor(0.2667)\n","6 tensor(0.2625)\n","7 tensor(0.2633)\n","8 tensor(0.2610)\n","9 tensor(0.2613)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MiK8w5J8IMNd","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OCbfL4ciFiRs","colab_type":"text"},"source":["# Create fit() and get_data()\n","----------------------------------\n","\n","We'll now do a little refactoring of our own. Since we go through a similar\n","process twice of calculating the loss for both the training set and the\n","validation set, let's make that into its own function, ``loss_batch``, which\n","computes the loss for one batch.\n","\n","We pass an optimizer in for the training set, and use it to perform\n","backprop.  For the validation set, we don't pass an optimizer, so the\n","method doesn't perform backprop.\n","\n"]},{"cell_type":"code","metadata":{"id":"Pq3AqOV2FiRt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600583179069,"user_tz":-540,"elapsed":727,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["def loss_batch(model, loss_func, xb, yb, opt=None):\n","    loss = loss_func(model(xb), yb)\n","\n","    if opt is not None:\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","    return loss.item(), len(xb)"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3PcXe6mFiRw","colab_type":"text"},"source":["``fit`` runs the necessary operations to train our model and compute the\n","training and validation losses for each epoch.\n","\n"]},{"cell_type":"code","metadata":{"id":"MC4ZRQoMFiRx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600583207707,"user_tz":-540,"elapsed":716,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["import numpy as np\n","\n","def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n","    for epoch in range(epochs):\n","        model.train()\n","        for xb, yb in train_dl:\n","            loss_batch(model, loss_func, xb, yb, opt)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            losses, nums = zip(\n","                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n","            )\n","        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n","\n","        print(epoch, val_loss)"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sBskeCiqFiR-","colab_type":"text"},"source":["``get_data`` returns dataloaders for the training and validation sets.\n","\n"]},{"cell_type":"code","metadata":{"id":"LiT1MlYSFiR-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600583210318,"user_tz":-540,"elapsed":732,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["def get_data(train_ds, valid_ds, bs):\n","    return (\n","        DataLoader(train_ds, batch_size=bs, shuffle=True),\n","        DataLoader(valid_ds, batch_size=bs * 2),\n","    )"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZtFJuv7FiSB","colab_type":"text"},"source":["Now, our whole process of obtaining the data loaders and fitting the\n","model can be run in 3 lines of code:\n","\n"]},{"cell_type":"code","metadata":{"id":"mafC-u2XFiSB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1600583220779,"user_tz":-540,"elapsed":7760,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"877a5448-23d3-49fc-90df-826ffabc3288"},"source":["train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","model, opt = get_model()\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["0 0.30329041845798493\n","1 0.2931728851675987\n","2 0.2822484631061554\n","3 0.27551506533622744\n","4 0.269929381275177\n","5 0.27011927086114884\n","6 0.2746586132347584\n","7 0.28049133939743043\n","8 0.26336252180337905\n","9 0.27337815339565275\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ew-xQoVrFiSE","colab_type":"text"},"source":["You can use these basic 3 lines of code to train a wide variety of models.\n","Let's see if we can use them to train a convolutional neural network (CNN)!\n","\n","# Switch to CNN\n","-------------\n","\n","We are now going to build our neural network with three convolutional layers.\n","Because none of the functions in the previous section assume anything about\n","the model form, we'll be able to use them to train a CNN without any modification.\n","\n","We will use Pytorch's predefined\n","`Conv2d <https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`_ class\n","as our convolutional layer. We define a CNN with 3 convolutional layers.\n","Each convolution is followed by a ReLU.  At the end, we perform an\n","average pooling.  (Note that ``view`` is PyTorch's version of numpy's\n","``reshape``)\n","\n"]},{"cell_type":"code","metadata":{"id":"CGNPlxIQFiSF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600583684408,"user_tz":-540,"elapsed":834,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["class Mnist_CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n","\n","    def forward(self, xb):\n","        xb = xb.view(-1, 1, 28, 28)\n","        xb = F.relu(self.conv1(xb))\n","        xb = F.relu(self.conv2(xb))\n","        xb = F.relu(self.conv3(xb))\n","        xb = F.avg_pool2d(xb, 4)\n","        return xb.view(-1, xb.size(1))\n","\n","lr = 0.1"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2PUo81GeFiSI","colab_type":"text"},"source":["`Momentum <https://cs231n.github.io/neural-networks-3/#sgd>`_ is a variation on\n","stochastic gradient descent that takes previous updates into account as well\n","and generally leads to faster training.\n","\n"]},{"cell_type":"code","metadata":{"id":"PvCeQO8_FiSJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1600583774022,"user_tz":-540,"elapsed":70782,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"041937e7-2c9e-4a31-d94d-a372035e1bc7"},"source":["model = Mnist_CNN()\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["0 0.6164914716243743\n","1 0.27454934899806976\n","2 0.27154313964247706\n","3 0.21197214735746384\n","4 0.20734049916863442\n","5 0.17061396770477294\n","6 0.17715126511454582\n","7 0.1654526520371437\n","8 0.15465553597211837\n","9 0.15092532858252525\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iPOzc4eiMUE8","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CSkMhpFWFiSP","colab_type":"text"},"source":["# Wrapping DataLoader\n","-----------------------------\n","\n","Our CNN is fairly concise, but it only works with MNIST, because:\n"," - It assumes the input is a 28\\*28 long vector\n"," - It assumes that the final CNN grid size is 4\\*4 (since that's the average\n","pooling kernel size we used)\n","\n","Let's get rid of these two assumptions, so our model works with any 2d\n","single channel image. First, we can remove the initial Lambda layer but\n","moving the data preprocessing into a generator:\n","\n"]},{"cell_type":"code","metadata":{"id":"p7Ri6E4jFiSQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600583848153,"user_tz":-540,"elapsed":732,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["class Lambda(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","\n","    def forward(self, x):\n","        return self.func(x)\n","\n","def preprocess(x, y):\n","    return x.view(-1, 1, 28, 28), y\n","\n","\n","class WrappedDataLoader:\n","    def __init__(self, dl, func):\n","        self.dl = dl\n","        self.func = func\n","\n","    def __len__(self):\n","        return len(self.dl)\n","\n","    def __iter__(self):\n","        batches = iter(self.dl)\n","        for b in batches:\n","            yield (self.func(*b))\n","\n","train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","train_dl = WrappedDataLoader(train_dl, preprocess)\n","valid_dl = WrappedDataLoader(valid_dl, preprocess)"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2QLgwrflFiSS","colab_type":"text"},"source":["Next, we can replace ``nn.AvgPool2d`` with ``nn.AdaptiveAvgPool2d``, which\n","allows us to define the size of the *output* tensor we want, rather than\n","the *input* tensor we have. As a result, our model will work with any\n","size input.\n","\n"]},{"cell_type":"code","metadata":{"id":"IeQPHGyOFiSS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600583856254,"user_tz":-540,"elapsed":851,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["model = nn.Sequential(\n","    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.AdaptiveAvgPool2d(1),\n","    Lambda(lambda x: x.view(x.size(0), -1)),\n",")\n","\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0YrzXNJyFiST","colab_type":"text"},"source":["Let's try it out:\n","\n"]},{"cell_type":"code","metadata":{"id":"GMYCFIGMFiSU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1600583933184,"user_tz":-540,"elapsed":70591,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"a9dd34ef-98dd-4b02-add4-85c70d52c95c"},"source":["fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["0 0.4348168075561523\n","1 0.29296421473026274\n","2 0.25236335500478746\n","3 0.22040123490691185\n","4 0.20860774806141855\n","5 0.1945936506807804\n","6 0.19299367569088935\n","7 0.20914872024655343\n","8 0.179460185110569\n","9 0.14217451072335244\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DDK8fEpVO4kO","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kFVqS0gFiSj","colab_type":"text"},"source":["# Closing thoughts\n","-----------------\n","\n","We now have a general data pipeline and training loop which you can use for\n","training many types of models using Pytorch. To see how simple training a model\n","can now be, take a look at the `mnist_sample` sample notebook.\n","\n","Of course, there are many things you'll want to add, such as data augmentation,\n","hyperparameter tuning, monitoring training, transfer learning, and so forth.\n","These features are available in the fastai library, which has been developed\n","using the same design approach shown in this tutorial, providing a natural\n","next step for practitioners looking to take their models further.\n","\n","We promised at the start of this tutorial we'd explain through example each of\n","``torch.nn``, ``torch.optim``, ``Dataset``, and ``DataLoader``. So let's summarize\n","what we've seen:\n","\n"," - **torch.nn**\n","\n","   + ``Module``: creates a callable which behaves like a function, but can also\n","     contain state(such as neural net layer weights). It knows what ``Parameter`` (s) it\n","     contains and can zero all their gradients, loop through them for weight updates, etc.\n","   + ``Parameter``: a wrapper for a tensor that tells a ``Module`` that it has weights\n","     that need updating during backprop. Only tensors with the `requires_grad` attribute set are updated\n","   + ``functional``: a module(usually imported into the ``F`` namespace by convention)\n","     which contains activation functions, loss functions, etc, as well as non-stateful\n","     versions of layers such as convolutional and linear layers.\n"," - ``torch.optim``: Contains optimizers such as ``SGD``, which update the weights\n","   of ``Parameter`` during the backward step\n"," - ``Dataset``: An abstract interface of objects with a ``__len__`` and a ``__getitem__``,\n","   including classes provided with Pytorch such as ``TensorDataset``\n"," - ``DataLoader``: Takes any ``Dataset`` and creates an iterator which returns batches of data.\n","\n"]}]}