{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[PyTorch] Tutorial_2","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMXZvOnrGa6BdTH6wzh5gDV"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"2QDWw0lX54ab","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599822174259,"user_tz":-540,"elapsed":1059,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"cd739382-fa64-4b5f-9874-1dfbb8f42404"},"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IhhUPshW54ae","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1599823178106,"user_tz":-540,"elapsed":1070,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"691a015c-46c0-4a65-e44d-b6cde2ba5a69"},"source":["y = x + 2\n","print(y)\n","print(x.backward)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n","<bound method Tensor.backward of tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bhBGjmys54ah","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599822269532,"user_tz":-540,"elapsed":1092,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"e8ad31c5-fb03-43c2-a740-9202bd8fe11b"},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"POGsk2POxypk","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OP9wqGEax1fk","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"63-u4GPYx38K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599821352685,"user_tz":-540,"elapsed":1235,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"a38c3bed-bab4-4341-82e4-fbdae50abe3a"},"source":["# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.\n","torch.manual_seed(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fb88e6c40c0>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"GEiMfs5v1Kwd","colab_type":"text"},"source":["# Auto grad\n","\n","Tensor\n","패키지의 중심에는 torch.Tensor 클래스가 있습니다. 만약 .requires_grad 속성을 True 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기 시작합니다. 계산이 완료된 후 .backward() 를 호출하여 모든 변화도(gradient)를 자동으로 계산할 수 있습니다. 이 Tensor의 변화도는 .grad 속성에 누적됩니다.\n","\n","Tensor가 기록을 추적하는 것을 중단하게 하려면, .detach() 를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있습니다.\n","\n","기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하기 위해, 코드 블럭을 with torch.no_grad(): 로 감쌀 수 있습니다. 이는 특히 변화도(gradient)는 필요없지만, requires_grad=True 가 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용합니다.\n","\n","Autograd 구현에서 매우 중요한 클래스가 하나 더 있는데, 이것은 바로 Function 클래스입니다.\n","\n","Tensor 와 Function 은 서로 연결되어 있으며, 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프(acyclic graph)를 생성합니다. 각 tensor는 .grad_fn 속성을 갖고 있는데, 이는 Tensor 를 생성한 Function 을 참조하고 있습니다. (단, 사용자가 만든 Tensor는 예외로, 이 때 grad_fn 은 None 입니다.)\n","\n","도함수를 계산하기 위해서는 Tensor 의 .backward() 를 호출하면 됩니다. 만약 Tensor 가 스칼라(scalar)인 경우(예. 하나의 요소 값만 갖는 등)에는 backward 에 인자를 정해줄 필요가 없습니다. 하지만 여러 개의 요소를 갖고 있을 때는 tensor의 모양을 gradient 의 인자로 지정할 필요가 있습니다."]},{"cell_type":"code","metadata":{"id":"4PLa5eSSyInz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599822174259,"user_tz":-540,"elapsed":1059,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"cd739382-fa64-4b5f-9874-1dfbb8f42404"},"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LCW8YYmX1RPW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1599823178106,"user_tz":-540,"elapsed":1070,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"691a015c-46c0-4a65-e44d-b6cde2ba5a69"},"source":["y = x + 2\n","print(y)\n","print(x.backward)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n","<bound method Tensor.backward of tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J_t0aHpW1Vn0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599822269532,"user_tz":-540,"elapsed":1092,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"e8ad31c5-fb03-43c2-a740-9202bd8fe11b"},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fmcpbtB71ofr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":158},"executionInfo":{"status":"ok","timestamp":1599822351030,"user_tz":-540,"elapsed":1049,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"ff2d4be0-1bca-407c-97e6-79877334bf09"},"source":["a = torch.randn(2, 2)\n","print(a)\n","print(a*3)\n","print(a-1)\n","a = ((a * 3) / (a - 1))\n","print(a) #elementwise"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([[ 1.5410, -0.2934],\n","        [-2.1788,  0.5684]])\n","tensor([[ 4.6230, -0.8803],\n","        [-6.5364,  1.7053]])\n","tensor([[ 0.5410, -1.2934],\n","        [-3.1788, -0.4316]])\n","tensor([[ 8.5453,  0.6806],\n","        [ 2.0562, -3.9514]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p7RJ4Idu2k7c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599822699621,"user_tz":-540,"elapsed":1083,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"4dc0f5fe-95cc-4cdd-ccd5-fffab554b571"},"source":["print(4.6230/0.5410)\n","print(-0.8803/-1.2934)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["8.5452865064695\n","0.6806092469460336\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r_MW6Nao17KT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1599823286860,"user_tz":-540,"elapsed":920,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"9fff312c-5df0-4e73-c344-e676328e2888"},"source":["print(a.requires_grad) # 기본값이 False 기때문에\n","a.requires_grad_(True) #a에 저장하게 함\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b)\n","print(b.grad_fn)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["True\n","True\n","tensor(93.3274, grad_fn=<SumBackward0>)\n","<SumBackward0 object at 0x7fb83bb534a8>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NA79rLaD4OS7","colab_type":"text"},"source":["Gradients\n","---------\n","Let's backprop now\n","Because ``out`` contains a single scalar, ``out.backward()`` is\n","equivalent to ``out.backward(torch.tensor(1))``.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Qvd1Ivfl6CFz","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599823960421,"user_tz":-540,"elapsed":1068,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"dd2a3a75-833f-4174-9394-775ef0473521"},"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mDEGcKs56CF9","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1599823962284,"user_tz":-540,"elapsed":1198,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"9290cf4b-7c28-4195-c214-a1a336459865"},"source":["y = x + 2\n","print(y)\n","print(x.backward)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n","<bound method Tensor.backward of tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IJbt_NYi6CGC","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599823966781,"user_tz":-540,"elapsed":1062,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"1c0d1ac7-b01c-495d-e49c-dde33a5aa66b"},"source":["z = y * y * 3\n","out = z.mean() \n","\n","print(z, out)\n","\n"],"execution_count":26,"outputs":[{"output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qcdK95LQ4OS-","colab_type":"text"},"source":["print gradients d(out)/dx\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"D-xfX4mP4OS8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599823977179,"user_tz":-540,"elapsed":1719,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}}},"source":["out.backward() # 첫 변수까지의 "],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nnt9V_z14OS-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"status":"ok","timestamp":1599824029742,"user_tz":-540,"elapsed":1039,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"daac1f0c-1bab-41e5-b770-664d2c335299"},"source":["print(x.grad)\n","print(y.grad)\n","print(z.grad)\n","#z=3(x+2)^2=3x^2+12x+12\n","#dz/dx = 6x+12\n","#dout/dx = dout/dz *dz/dy *dy/dx\n","#dout/dx의 size = (2,2) 가 될거야 \n","#dout/dx1= dout/dz1 *dz1/dy1 *dy1/dx1\n","        #= 1/4 * 6y1 * 1\n","        #= 1/4 * 6*3 * 1\n","        # 4.5 \n"],"execution_count":29,"outputs":[{"output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n","None\n","None\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"U6OqmgjY4OTB","colab_type":"text"},"source":["You should have got a matrix of ``4.5``. Let’s call the ``out``\n","*Tensor* “$o$”.\n","We have that $o = \\frac{1}{4}\\sum_i z_i$,\n","$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n","Therefore,\n","$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n","$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Al_4P_bH4OTC","colab_type":"text"},"source":["You can do many crazy things with autograd!\n","\n"]},{"cell_type":"code","metadata":{"id":"01_a53m84OTC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"ok","timestamp":1599826836205,"user_tz":-540,"elapsed":1047,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"d9a0567b-8942-476e-c3aa-23eb602b6d2f"},"source":["torch.manual_seed(0)\n","x = torch.randn(3, requires_grad=True)\n","print('------------------x-------------------')\n","print(f'x=\\n{x}')\n","\n","y = x * 2\n","\n","print('------------------y-------------------')\n","print(f'y=\\n{y}')\n","print(y.data.norm()) # 벡터의 크기 sqrt(sum(squared))\n","print(4*x.data.norm()) # y=2x ㅣyㅣ=sqrt(4x^2)=2ㅣxㅣ\n","while y.data.norm() < 1000:\n","    y = y * 2\n","print('-------after y norm--------------')\n","print(f'y=\\n{y}')\n"],"execution_count":85,"outputs":[{"output_type":"stream","text":["------------------x-------------------\n","x=\n","tensor([ 1.5410, -0.2934, -2.1788], requires_grad=True)\n","------------------y-------------------\n","y=\n","tensor([ 3.0820, -0.5869, -4.3576], grad_fn=<MulBackward0>)\n","tensor(5.3695)\n","tensor(10.7390)\n","-------after y norm--------------\n","y=\n","tensor([  788.9900,  -150.2356, -1115.5402], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QQqz3pn24OTG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599826852416,"user_tz":-540,"elapsed":1175,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"e3a9be40-09b5-4185-908c-2d468aa64572"},"source":["original=torch.tensor([1,1,1],dtype=torch.long)\n","gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","# x에 대한 y의 미분값에 gradients 내 원소들이 \n","# elementwise로 곱해짐\n","# y.backward() 이건 y가 scalar 값일 때 가능\n","# jacobian 형태를 유지하려면 저렇게 [1,1,1]을 y.backward([1,1,1]) 해줘야함\n","y.backward(original)\n","print(f'x.grad=\\n{x.grad}')\n"],"execution_count":86,"outputs":[{"output_type":"stream","text":["x.grad=\n","tensor([512., 512., 512.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dlsbM1HRHI02","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"executionInfo":{"status":"ok","timestamp":1599826874134,"user_tz":-540,"elapsed":1160,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"1fb1596d-2aa2-42b4-ce94-864db1dacad7"},"source":["torch.manual_seed(0)\n","x = torch.randn(3, requires_grad=True)\n","print('------------------x-------------------')\n","print(f'x=\\n{x}')\n","\n","y = x * 2\n","\n","print('------------------y-------------------')\n","print(f'y=\\n{y}')\n","print(y.data.norm()) # 벡터의 크기 sqrt(sum(squared))\n","print(4*x.data.norm()) # y=2x ㅣyㅣ=sqrt(4x^2)=2ㅣxㅣ\n","while y.data.norm() < 1000:\n","    y = y * 2\n","print('-------after y norm--------------')\n","print(f'y=\\n{y}')\n","\n","y.backward(gradients)\n","print(f'x.grad=\\n{x.grad}')"],"execution_count":87,"outputs":[{"output_type":"stream","text":["------------------x-------------------\n","x=\n","tensor([ 1.5410, -0.2934, -2.1788], requires_grad=True)\n","------------------y-------------------\n","y=\n","tensor([ 3.0820, -0.5869, -4.3576], grad_fn=<MulBackward0>)\n","tensor(5.3695)\n","tensor(10.7390)\n","-------after y norm--------------\n","y=\n","tensor([  788.9900,  -150.2356, -1115.5402], grad_fn=<MulBackward0>)\n","x.grad=\n","tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mvHzke744OTI","colab_type":"text"},"source":["You can also stop autograd from tracking history on Tensors\n","with ``.requires_grad=True`` by wrapping the code block in\n","``with torch.no_grad()``:\n","\n"]},{"cell_type":"code","metadata":{"id":"igo-Tmcp4OTI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1599824862807,"user_tz":-540,"elapsed":1105,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"9b0d5a01-5b02-40aa-ec63-5b34ef91b7c7"},"source":["print(x.requires_grad)\n","print((x ** 2).requires_grad)\n","\n","with torch.no_grad():\n","\tprint((x ** 2).requires_grad)"],"execution_count":52,"outputs":[{"output_type":"stream","text":["True\n","True\n","False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tzJXKY354OTK","colab_type":"text"},"source":["**Read Later:**\n","\n","Documentation of ``autograd`` and ``Function`` is at\n","http://pytorch.org/docs/autograd\n","\n"]},{"cell_type":"code","metadata":{"id":"NpBeiQGN3bA-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1599826061932,"user_tz":-540,"elapsed":1485,"user":{"displayName":"‍김도윤[ 학부재학 / 산업경영공학부 ]","photoUrl":"","userId":"10791284398836833218"}},"outputId":"f0a9867e-f975-4d83-b9f3-0c9c858933ce"},"source":["from torch.autograd import Variable\n","a=torch.rand(5)\n","a=Variable(a,requires_grad=True)\n","b=a+2\n","b.requires_grad_(True)\n","\n","c=b**(1/2)\n","c.requires_grad_(True) \n","\n","out=c.sum()\n","out.requires_grad_(True)\n","\n","#b.backward() :grad can be implicitly created \n","              #only for scalar outputs\n","#중간 과정의 미분값은 requires_grad_(True)를 해줘야\n","             #할수 있음\n","out.backward() #out을 가지고 a까지의 미분값을 구한다는 거야\n","\n","print('--------------a----------------')\n","print(a.data)\n","print(a.grad)\n","print(a.grad_fn)\n","\n","print('--------------b----------------')\n","print(b)\n","print(b.data)\n","print(b.grad)\n","print(b.grad_fn)\n","\n","print('--------------c----------------')\n","print(c)\n","print(c.data)\n","print(c.grad)\n","print(c.grad_fn)\n","\n","print('--------------out----------------')\n","print(out)\n","print(out.data)\n","print(out.grad)\n","print(out.grad_fn)"],"execution_count":69,"outputs":[{"output_type":"stream","text":["--------------a----------------\n","tensor([0.1143, 0.4725, 0.5751, 0.2952, 0.7967])\n","tensor([0.3439, 0.3180, 0.3116, 0.3300, 0.2990])\n","None\n","--------------b----------------\n","tensor([2.1143, 2.4725, 2.5751, 2.2952, 2.7967], grad_fn=<AddBackward0>)\n","tensor([2.1143, 2.4725, 2.5751, 2.2952, 2.7967])\n","None\n","<AddBackward0 object at 0x7fb83ba19390>\n","--------------c----------------\n","tensor([1.4541, 1.5724, 1.6047, 1.5150, 1.6723], grad_fn=<PowBackward0>)\n","tensor([1.4541, 1.5724, 1.6047, 1.5150, 1.6723])\n","None\n","<PowBackward0 object at 0x7fb83ba2b630>\n","--------------out----------------\n","tensor(7.8185, grad_fn=<SumBackward0>)\n","tensor(7.8185)\n","None\n","<SumBackward0 object at 0x7fb83ba2b668>\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"V8xTB2UTAwTP","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}